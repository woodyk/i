config_version: "1.0"
default_action: "identify"
logging:
  enabled: true
  level: "info"
  log_file: "/tmp/i.log"
paths:
  data_dir: "data/"
  temp_dir: "/tmp/i/"
  default_output_dir: "./"
includes:
  - "config/default/prompts.yaml"
  - "config/default/actions.yaml"
llm:
  openai:
    provider: "openai"
    max_tokens: 1024
    temperature: 0.7
    api_key: "your-openai-key"
  ollama:
    provider: "ollama"
    max_tokens: 1024
    temperature: 0.7
    host: "http://localhost:1143"
    api_key: null
